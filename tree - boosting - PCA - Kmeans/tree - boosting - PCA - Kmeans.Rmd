---
title: "hw4"
author: "fabrizio dimino"
date: "2023-11-28"
output: word_document
---
##Problem 1
```{r setup}
setwd('C:/Users/fdimino/OneDrive/Desktop/Courses/FA582 data science/labs/forth')
oj <- read.csv('OJ.csv')

# a) Create a training set containing a random sample of 800 observations, and a test set 
# containing the remaining observations.
set.seed(1)
train_index <- sample(1:nrow(oj), 800)
training <- oj[train_index, ]
oj.test <- oj[-train_index, ]

# b) Fit a tree to the training data, with Purchase as the response and the other variables as 
# predictors. Use the summary() function to produce summary statistics about the tree, 
# and describe the results obtained. What is the training error rate? How many terminal 
# nodes does the tree have?
library(tree)
tree_model <- tree(as.factor(Purchase) ~ ., data = training)
summary(tree_model)
# The tree has 9 nodes. Terminal nodes are the end points of the tree, representing final decision points.
# The residual mean deviance is a measure of model goodness-of-fit. 
# A lower deviance value indicates a better fit of the model to the data.
# The misclassification error rate, also known as the training error rate, is 0.1588 or 15.88%.
# Display Tree Information


# c) and d)
# Type in the name of the tree object in order to get a detailed text output. Pick one of 
# the terminal nodes, and interpret the information displayed.
# plot and interpret results
plot(tree_model)
text(tree_model, pretty = 0)
tree_model
# The most important attribute for the first division is LoyalCH, indicating that customer loyalty is crucial in predicting purchase type.
# then, The tree divides the observations according to different variables such as LoyalCH, PriceDiff, SpecialCH, ListPriceDiff, and PctDiscMM. This suggests that these variables significantly influence purchase decisions.
# The terminal nodes show the final predictions based on the conditions encountered. For example, node 8 shows that with a very low value of LoyalCH, the purchase is almost always predicted as 'MM' with high confidence (98.3%).
# Tree shows some tendency toward the 'MM' (Minute Maid) class over the 'CH' (Citrus Hill) class, suggesting that under some specific conditions, the model more frequently predicts 'MM'.


# e)
# Predict the response on the test data, and produce a confusion matrix comparing the
# test labels to the predicted test labels. What is the test error rate?
tree.pred <- predict(tree_model,oj.test,type="class")
conf.matrix <- table(tree.pred,oj.test$Purchase)
conf.matrix
summary(tree.pred)
test.error <- mean(tree.pred != oj.test$Purchase)
test.error

# f)
# Apply the cv.tree() function to the training set in order to determine the optimal tree size
cv.oj <- cv.tree(tree_model,FUN=prune.misclass)
optimal_tree_size <- cv.oj$size[which.min(cv.oj$dev)]

# g)
#  Produce a plot with tree size on the x-axis and cross-validated classification error rate on the y-axis.
plot(cv.oj$size,cv.oj$dev,type="b")
```


##Problem 2
```{r setup1}
caravan <- read.csv("CARAVAN.csv")

#a)
# Create a training set consisting of the first 1,000 observations, and a test set consisting
# of the remaining observations.
set.seed(1)

train_index <- sample(1:nrow(caravan), 1000)
training <- caravan[train_index,]
test <- caravan[-train_index,]
training$Purchase <- ifelse(training$Purchase == "Yes", 1, 0)


#b)
# Fit a boosting model to the training set with Purchase as the response and the other
# variables as predictors. Use 1,000 trees, and a shrinkage value of 0.01. Which predictors
# appear to be the most important?
library(gbm)
boost.caravan <- gbm(Purchase~.,data=training,distribution='bernoulli',n.trees=1000, shrinkage = 0.01)
summary(boost.caravan, plotit = FALSE)
# PPERSAUT, MAUT2, ALEVEN appear to be the most important.

#c)
# Use the boosting model to predict the response on the test data. Predict that a person
# will make a purchase if the estimated probability of purchase is greater than 20 %. Form
# a confusion matrix. What fraction of the people predicted to make a purchase do in fact
# make one? How does this compare with the results obtained from applying KNN or
# logistic regression to this data set?

yhat.boost <- predict(boost.caravan,newdata=test,n.trees=1000, type = "response")
summary(yhat.boost)

pred_purchase <- ifelse(yhat.boost > 0.2, 1, 0)
conf_matrix <- table(pred_purchase, test$Purchase)
conf_matrix
fraction <- 40/296
fraction

# The fraction of the people predicted to make a purchase do in fact make one is 13.5%

glm.fit <- glm(as.factor(Purchase) ~ ., data = training, family = binomial)
summary(glm.fit)
## perche mi escono dati mancanti
glm.probs <- predict(glm.fit,newdata = test, type="response")
glm.pred=rep("No",dim(test)[1])
glm.pred[glm.probs>0.2]="Yes"

table(glm.pred,test$Purchase)
fraction <- 52/(244+52)
fraction

# KNN
library(class)

# Fit KNN model
knn.pred <- knn(train = as.matrix(training[, -which(names(training) == "Purchase")]),
                 test = as.matrix(test[, -which(names(test) == "Purchase")]),
                 cl = training$Purchase,
                 k = 5) 
knn_confusion_matrix <- table(Predicted = knn.pred, Actual = test$Purchase)
knn_confusion_matrix
fraction <- 7/(289+7)
fraction
```


##Problem 3
```{r setup2}
# a) Generate a simulated data set with 20 observations in each of three classes (i.e. 60
# observations total), and 50 variables. Hint: There are a number of functions in R that you
# can use to generate data. One example is the rnorm() function; runif() is another option.
# Be sure to add a mean shift to the observations in each class so that there are three
# distinct classes.

# Number of observations in each class
n_per_class <- 20

# Number of variables
n_variables <- 50

# Generate data for each class with distinct means
class_1 <- matrix(rnorm(n_per_class * n_variables, mean = 0, sd = 1), ncol = n_variables)
class_2 <- matrix(rnorm(n_per_class * n_variables, mean = 3, sd = 1), ncol = n_variables)
class_3 <- matrix(rnorm(n_per_class * n_variables, mean = 6, sd = 1), ncol = n_variables)

# Combine data for all classes
simulated_data <- rbind(class_1, class_2, class_3)

# Create labels for classes (1, 2, 3)
labels <- rep(1:3, each = n_per_class)

# Convert to data frame and add class labels
simulated_df <- as.data.frame(simulated_data)
simulated_df$Class <- as.factor(labels)

# View the structure of the generated dataset
str(simulated_df)
tail(simulated_df)

apply(simulated_df, 2, mean)
apply(simulated_df, 2, sd)


## b) Perform PCA on the 60 observations and plot the first two principal component score
# vectors. Use a different color to indicate the observations in each of the three classes. If
# the three classes appear separated in this plot, then continue on to part (c). If not, then
# return to part (a) and modify the simulation so that there is greater separation between
# the three classes. Do not continue to part (c) until the three classes show at least some
# separation in the first two principal component score vectors.

pca <- prcomp(simulated_df[,-51], scale=TRUE)
names(pca)

pc1 <- pca$x[, 1]
pc2 <- pca$x[, 2]

par(mar = c(2, 2, 2, 2))
plot(pc1, pc2, col = labels, pch = 19,
     xlab = "Principal Component 1", ylab = "Principal Component 2",
     main = "PCA of Simulated Data with Three Classes")
legend("topright", legend = unique(labels), col = 1:length(unique(labels)), pch = 19, title = "Classes")


#c)  Perform K-means clustering of the observations with K = 3. How well do the clusters that 
# you obtained in K-means clustering compare to the true class labels? Hint: You can use 
# the table() function in R to compare the true class labels to the class labels obtained by 
# clustering. Be careful how you interpret the results: K-means clustering will arbitrarily 
# number the clusters, so you cannot simply check whether the true class labels and 
# clustering labels are the same.

kmeans_result <- kmeans(simulated_df[,-51], centers = 3)
table(true_labels = simulated_df$Class, kmeans_cluster = kmeans_result$cluster)

# the table displays a perfect clustering scenario where each true class label aligns perfectly with a distinct cluster obtained from K-means clustering.
# In this ideal scenario, each true class label (1, 2, 3) is entirely allocated to a separate cluster (1, 2, 3) without any mixing.


# d)  Perform K-means clustering with K = 2. Describe your results
kmeans_result <- kmeans(simulated_df[,-51], centers = 2)
table(true_labels = simulated_df$Class, kmeans_cluster = kmeans_result$cluster)

# in this specific case, the clustering labels align perfectly with only two out of the three true class labels. 
# Cluster 1 contains all observations from the true labels 1 and 2
# Cluster 2 contains all observations from the true label 3
# This outcome suggests that with K=2, the K-means algorithm successfully differentiates between two classes but fails to capture the characteristics of the third class.
# Moreover, The proportion "(between_SS / total_SS = 65.5 %)" indicates the percentage of variation between clusters compared with the total variation


# e)
#  Now perform K-means clustering with K = 4, and describe your results
kmeans_result <- kmeans(simulated_df[,-51], centers = 4)
table(true_labels = simulated_df$Class, kmeans_cluster = kmeans_result$cluster)

# These results show that clustering with K=4 divided the observations primarily on the basis of distinguishing between class 1 and the other classes, 
# creating a cluster containing a combination of observations from classes 1 and 2. 
# The other clusters correspond accurately to the true classes 2 and 3, each containing all observations from the respective class.

# f) 
# Now perform K-means clustering with K = 3 on the first two principal component score 
# vectors, rather than on the raw data. That is, perform K-means clustering on the 60 Ã— 2 
# matrix of which the first column is the first principal component score vector, and the 
# second column is the second principal component score vector. Comment on the 
# results

pc_scores <- pca$x[, 1:2] 
kmeans_result <- kmeans(pc_scores, centers = 3)
table(true_labels = simulated_df$Class, kmeans_cluster = kmeans_result$cluster)

# kmeans_cluster do not align well with the true class labels for classes 1 and 2. 
# For classes 1 and 2, the clustering labels do not correspond to the true class labels. In both cases, none of the observations are correctly clustered into their respective classes 
# For class 3, the clustering labels seem to match more accurately with the true class labels. Nine observations from true class 3 are clustered into cluster 1, while 11 observations from true class 3 are clustered into cluster 2.
# Overall, the clustering labels based on the first two principal component score vectors do not seem to capture the underlying structure of classes 1 and 2 well. 

# g)
# Using the scale() function, perform K-means clustering with K = 3 on the data after 
# scaling each variable to have standard deviation one. How do these results compare to 
# those obtained in (b)? Explain.
scaled_data <- as.data.frame(scale(simulated_df[, -51]))
kmeans_result_scaled <- kmeans(scaled_data, centers = 3)
table(true_labels = simulated_df$Class, kmeans_cluster = kmeans_result_scaled$cluster)

# The results of K-means clustering show a perfect separation between the three true classes.
# All observations from true class 1 are correctly clustered into cluster 2.
# All observations from true class 2 are correctly clustered into cluster 1.
# All observations from true class 3 are correctly clustered into cluster 3.
# These results suggest that scaling the variables to have a standard deviation of one has led to a perfect separation of the classes with K-means clustering.
```



